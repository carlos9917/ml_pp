{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db7998ab-fdef-4c1c-93c8-4e8ce91eaef1",
   "metadata": {},
   "source": [
    "### Example of a crude data-driven model based on LSTMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "a466f44e-cca0-48e1-ac02-eeeb5971ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sqlite3\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8988d6b9-2394-403e-a80d-c8fd43f7b5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, features, targets, timestamps, sequence_length):\n",
    "        self.features = torch.FloatTensor(features)\n",
    "        self.targets = torch.FloatTensor(targets)\n",
    "        self.timestamps = timestamps\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.features[idx:idx+self.sequence_length],\n",
    "            self.targets[idx+self.sequence_length],\n",
    "            self.timestamps[idx+self.sequence_length]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db06a56a-802a-41ff-aec6-7a6ea04f327f",
   "metadata": {},
   "source": [
    "#class WeatherDataset(Dataset):\n",
    "#    def __init__(self, features, targets, sequence_length):\n",
    "#        self.features = features\n",
    "#       self.targets = targets\n",
    "#        self.sequence_length = sequence_length#\n",
    "#\n",
    "#    def __len__(self):\n",
    "#        return len(self.features) - self.sequence_length#\n",
    "#\n",
    "#    def __getitem__(self, idx):\n",
    "#        return (\n",
    "#            self.features[idx:idx+self.sequence_length],\n",
    "#            self.targets[idx+self.sequence_length]\n",
    "#        )\n",
    "#Original version had an issue with the float32 data length\n",
    "\n",
    "class WeatherDataset(Dataset):\n",
    "    def __init__(self, features, targets, sequence_length):\n",
    "        self.features = torch.FloatTensor(features)  # Convert to torch.float32\n",
    "        self.targets = torch.FloatTensor(targets)    # Convert to torch.float32\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features) - self.sequence_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (\n",
    "            self.features[idx:idx+self.sequence_length],\n",
    "            self.targets[idx+self.sequence_length]\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1d77ce00-9877-46c6-9d68-a0fc2e4ac28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15c01627-95a2-4683-a4b7-a970a4cb2879",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_observation_data(db_path,variables, year):\n",
    "    dfs = []\n",
    "    for var in variables:\n",
    "        dbase = os.path.join(db_path,f\"OBSTABLE_{var}_{year}.sqlite\")\n",
    "        conn = sqlite3.connect(dbase)\n",
    "        query = f\"SELECT * FROM SYNOP\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        df['variable'] = var\n",
    "        dfs.append(df)\n",
    "        conn.close()\n",
    "\n",
    "    obs_data = pd.concat(dfs, axis=0)\n",
    "    obs_data['valid_dttm'] = pd.to_datetime(obs_data['valid_dttm'], unit='s')\n",
    "    return obs_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4887d011-95ed-497d-b9ca-3871c9db9338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_forecast_data(db_path, variables, year, month, init_time):\n",
    "    dfs = []\n",
    "    for var in variables:\n",
    "        dbase = os.path.join(db_path,f\"{year}/{month:02d}/FCTABLE_{var}_{year}{month:02d}_{init_time:02d}.sqlite\")\n",
    "        conn = sqlite3.connect(dbase)\n",
    "        query = f\"SELECT * FROM FC\"\n",
    "        df = pd.read_sql_query(query, conn)\n",
    "        dfs.append(df)\n",
    "        conn.close()\n",
    "\n",
    "    fc_data = pd.concat(dfs, axis=0)\n",
    "    fc_data['valid_dttm'] = pd.to_datetime(fc_data['valid_dttm'], unit='s')\n",
    "    fc_data['fcst_dttm'] = pd.to_datetime(fc_data['fcst_dttm'], unit='s')\n",
    "    return fc_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c91b3c58-7c5f-4d84-9179-0cf45a23819b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(obs_data, fc_data):\n",
    "    # Rename columns to avoid conflicts\n",
    "    obs_data = obs_data.rename(columns={'lat': 'lat_obs', 'lon': 'lon_obs', 'elev': 'elev_obs'})\n",
    "    fc_data = fc_data.rename(columns={'lat': 'lat_fc', 'lon': 'lon_fc', 'model_elevation': 'elev_fc'})\n",
    "\n",
    "    # Merge observation and forecast data\n",
    "    merged_data = pd.merge(obs_data, fc_data, on=['SID', 'valid_dttm'], suffixes=('_obs', '_fc'))\n",
    "\n",
    "    # Sort data by station and time\n",
    "    merged_data = merged_data.sort_values(['SID', 'valid_dttm'])\n",
    "\n",
    "    # Create features and target\n",
    "    feature_columns = ['TROAD', 'glatmodel_det', 'elev_obs', 'elev_fc', 'lat_obs', 'lon_obs', 'lead_time']\n",
    "    features = merged_data[feature_columns].values\n",
    "    target = merged_data['TROAD'].values\n",
    "    timestamps = merged_data['valid_dttm'].values\n",
    "\n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "    return scaled_features, target, timestamps, scaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bef1949-cdac-4dda-9bb0-ab589c5490ed",
   "metadata": {},
   "source": [
    "#def preprocess_data(obs_data, fc_data):\n",
    "#    # Merge observation and forecast data\n",
    "#    merged_data = pd.merge(obs_data, fc_data, on=['SID', 'valid_dttm'], suffixes=('_obs', '_fc'))###\n",
    "\n",
    "#    # Sort data by station and time\n",
    "#    merged_data = merged_data.sort_values(['SID', 'valid_dttm'])#\n",
    "\n",
    "#    # Create features and target\n",
    "#    features = merged_data[['TROAD', 'glatmodel_det', 'elev', 'model_elevation', 'lat', 'lon', 'lead_time']].values\n",
    "#    target = merged_data['TROAD'].values#\n",
    "\n",
    "#    # Normalize the data\n",
    "#    scaler = StandardScaler()\n",
    "#    scaled_features = scaler.fit_transform(features)\n",
    "#\n",
    "#    return scaled_features, target, scaler\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(obs_data, fc_data):\n",
    "    # Rename columns to avoid conflicts\n",
    "    obs_data = obs_data.rename(columns={'lat': 'lat_obs', 'lon': 'lon_obs', 'elev': 'elev_obs'})\n",
    "    fc_data = fc_data.rename(columns={'lat': 'lat_fc', 'lon': 'lon_fc', 'model_elevation': 'elev_fc'})\n",
    "    \n",
    "    # Merge observation and forecast data\n",
    "    merged_data = pd.merge(obs_data, fc_data, on=['SID', 'valid_dttm'], suffixes=('_obs', '_fc'))\n",
    "    \n",
    "    # Sort data by station and time\n",
    "    merged_data = merged_data.sort_values(['SID', 'valid_dttm'])\n",
    "    \n",
    "    # Create features and target\n",
    "    feature_columns = ['TROAD', 'glatmodel_det', 'elev_obs', 'elev_fc', 'lat_obs', 'lon_obs', 'lead_time']\n",
    "    features = merged_data[feature_columns].values\n",
    "    target = merged_data['TROAD'].values\n",
    "    \n",
    "    # Normalize the data\n",
    "    scaler = StandardScaler()\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    return scaled_features, target, scaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ea3a1d-7156-4f9e-9560-5b53c07ee027",
   "metadata": {},
   "source": [
    "#older version\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d59cbbd7-304e-415d-b377-494743ea0c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_features, batch_targets in train_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_features)\n",
    "            loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for batch_features, batch_targets in val_loader:\n",
    "                batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "                \n",
    "                outputs = model(batch_features)\n",
    "                loss = criterion(outputs, batch_targets.unsqueeze(1))\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {train_loss/len(train_loader):.4f}, Val Loss: {val_loss/len(val_loader):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4372aa0-1d8e-4a54-a0b6-8e7a0ded8c63",
   "metadata": {},
   "source": [
    "#older version\n",
    "def evaluate_model(model, test_loader, scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in test_loader:\n",
    "            outputs = model(batch_features)\n",
    "            predictions.extend(outputs.numpy().flatten())\n",
    "            actuals.extend(batch_targets.numpy().flatten())\n",
    "\n",
    "    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))[:, 0]\n",
    "    actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1))[:, 0]\n",
    "\n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "\n",
    "    return mae, rmse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "02567f47-3835-4d69-9ca8-03c2e59b1153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, scaler):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_features, batch_targets in test_loader:\n",
    "            batch_features, batch_targets = batch_features.to(device), batch_targets.to(device)\n",
    "            \n",
    "            outputs = model(batch_features)\n",
    "            predictions.extend(outputs.cpu().numpy().flatten())\n",
    "            actuals.extend(batch_targets.cpu().numpy().flatten())\n",
    "    \n",
    "    predictions = scaler.inverse_transform(np.array(predictions).reshape(-1, 1))[:, 0]\n",
    "    actuals = scaler.inverse_transform(np.array(actuals).reshape(-1, 1))[:, 0]\n",
    "    \n",
    "    mae = np.mean(np.abs(predictions - actuals))\n",
    "    rmse = np.sqrt(np.mean((predictions - actuals)**2))\n",
    "    \n",
    "    return mae, rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0e7bc124-0fe3-4de6-b06c-ef8e94817ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_based_split(dataset, train_ratio=0.7, val_ratio=0.15):\n",
    "    total_size = len(dataset)\n",
    "    train_size = int(train_ratio * total_size)\n",
    "    val_size = int(val_ratio * total_size)\n",
    "\n",
    "    train_dataset = Subset(dataset, range(train_size))\n",
    "    val_dataset = Subset(dataset, range(train_size, train_size + val_size))\n",
    "    test_dataset = Subset(dataset, range(train_size + val_size, total_size))\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fa65f122-d695-41ad-b816-b1976898b631",
   "metadata": {},
   "outputs": [],
   "source": [
    "#variables = ['TROAD', 'T2M', 'WS', 'WD', 'RH', 'PREC']  # Add all your variables here\n",
    "\n",
    "variables = ['TROAD', 'T2m', 'Td2m', 'D10m', 'S10m', 'AccPcp12h']  # Add all your variables here\n",
    "\n",
    "OB_PATH = \"/media/cap/extra_work/road_model/OBSTABLE\"\n",
    "FC_PATH = \"/media/cap/extra_work/road_model/FCTABLE\"\n",
    "\n",
    "year = 2023  # Change this to the year you want to analyze\n",
    "month = 1  # Change this to the month you want to analyze\n",
    "init_time = 0  # Change this to the initialization time you want to use\n",
    "\n",
    "# Load observation and forecast data\n",
    "obs_data = load_observation_data(OB_PATH,variables, year)\n",
    "fc_data = load_forecast_data(FC_PATH,variables, year, month, init_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2881a70b-d40a-42bf-9410-6a0b3f7adb15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>valid_dttm</th>\n",
       "      <th>SID</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>elev</th>\n",
       "      <th>TROAD</th>\n",
       "      <th>variable</th>\n",
       "      <th>T2m</th>\n",
       "      <th>Td2m</th>\n",
       "      <th>D10m</th>\n",
       "      <th>S10m</th>\n",
       "      <th>AccPcp12h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>456182</th>\n",
       "      <td>2023-12-20 13:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456183</th>\n",
       "      <td>2023-12-20 14:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456184</th>\n",
       "      <td>2023-12-20 15:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456185</th>\n",
       "      <td>2023-12-20 08:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456186</th>\n",
       "      <td>2023-12-20 09:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456187</th>\n",
       "      <td>2023-12-20 10:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456188</th>\n",
       "      <td>2023-12-20 11:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>188.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456189</th>\n",
       "      <td>2023-12-06 10:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>171.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456190</th>\n",
       "      <td>2023-12-10 10:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>177.11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>456191</th>\n",
       "      <td>2023-12-17 18:00:00</td>\n",
       "      <td>130000</td>\n",
       "      <td>55.782998</td>\n",
       "      <td>12.280158</td>\n",
       "      <td>23.308584</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AccPcp12h</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>180.56</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                valid_dttm     SID        lat        lon       elev  TROAD  \\\n",
       "456182 2023-12-20 13:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456183 2023-12-20 14:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456184 2023-12-20 15:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456185 2023-12-20 08:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456186 2023-12-20 09:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456187 2023-12-20 10:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456188 2023-12-20 11:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456189 2023-12-06 10:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456190 2023-12-10 10:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "456191 2023-12-17 18:00:00  130000  55.782998  12.280158  23.308584    NaN   \n",
       "\n",
       "         variable  T2m  Td2m  D10m  S10m  AccPcp12h  \n",
       "456182  AccPcp12h  NaN   NaN   NaN   NaN     188.54  \n",
       "456183  AccPcp12h  NaN   NaN   NaN   NaN     188.54  \n",
       "456184  AccPcp12h  NaN   NaN   NaN   NaN     188.54  \n",
       "456185  AccPcp12h  NaN   NaN   NaN   NaN     188.02  \n",
       "456186  AccPcp12h  NaN   NaN   NaN   NaN     188.02  \n",
       "456187  AccPcp12h  NaN   NaN   NaN   NaN     188.02  \n",
       "456188  AccPcp12h  NaN   NaN   NaN   NaN     188.07  \n",
       "456189  AccPcp12h  NaN   NaN   NaN   NaN     171.56  \n",
       "456190  AccPcp12h  NaN   NaN   NaN   NaN     177.11  \n",
       "456191  AccPcp12h  NaN   NaN   NaN   NaN     180.56  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_data.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a16c0a7f-e713-4761-af39-207b84c238ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#obs_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e16c8d96-1203-4f1d-a93b-cafd61c6677c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocess data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m features, target, scaler \u001b[38;5;241m=\u001b[39m preprocess_data(obs_data, fc_data)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "features, target, scaler = preprocess_data(obs_data, fc_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fd7f002b-d6bb-420c-8887-0b6a9516e36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "sequence_length = 24  # Adjust as needed, this are the number of days\n",
    "dataset = WeatherDataset(features, target, sequence_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "005c5fce-2615-43ea-a0aa-ea55f32315bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train, validation, and test sets\n",
    "#train_size = int(0.7 * len(dataset))\n",
    "#val_size = int(0.15 * len(dataset))\n",
    "#test_size = len(dataset) - train_size - val_size\n",
    "#train_dataset, val_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, val_size, test_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cd225f-bb76-412d-8985-7dd7cf0d37b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Split data into train, validation, and test sets based on time\n",
    "    train_dataset, val_dataset, test_dataset = time_based_split(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ec63664b-850e-45ff-8e04-d89bcd686263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data loaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b80a6fe-8666-4038-9cb3-8e76333c69b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.Subset at 0x7e1e5a861310>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7c13132f-14ac-4d35-a9d6-f132ae6b8c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: nan, Val Loss: nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m output_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTM(input_size, hidden_size, num_layers, output_size)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[41], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     12\u001b[0m batch_features, batch_targets \u001b[38;5;241m=\u001b[39m batch_features\u001b[38;5;241m.\u001b[39mto(device), batch_targets\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_targets\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/media/cap/extra_work/miniforge3/envs/dataviz/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/cap/extra_work/miniforge3/envs/dataviz/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[37], line 12\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m h0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m c0 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhidden_size)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 12\u001b[0m out, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc0\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(out[:, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, :])\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m/media/cap/extra_work/miniforge3/envs/dataviz/lib/python3.9/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/media/cap/extra_work/miniforge3/envs/dataviz/lib/python3.9/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/media/cap/extra_work/miniforge3/envs/dataviz/lib/python3.9/site-packages/torch/nn/modules/rnn.py:917\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    914\u001b[0m         hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 917\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_first\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    920\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, batch_sizes, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias,\n\u001b[1;32m    921\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize and train the model\n",
    "input_size = features.shape[1]\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "output_size = 1\n",
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, output_size)\n",
    "train_model(model, train_loader, val_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df1398-4241-4fd1-b26c-e2c22db36a93",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
